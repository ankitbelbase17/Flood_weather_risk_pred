{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b134023",
   "metadata": {},
   "source": [
    "# üåä LSTM Training API Notebook - Heatwave/Flood Prediction\n",
    "\n",
    "This notebook exposes LSTM model training as a **REST API** that can be triggered by Airflow or other orchestration tools.\n",
    "\n",
    "**Features:**\n",
    "- Flask API with `/train` endpoint for triggering training\n",
    "- Public URL via ngrok for remote access from Airflow\n",
    "- Loads training data from HDFS via Cloudflare tunnels\n",
    "- Saves trained PyTorch models back to HDFS\n",
    "\n",
    "**API Endpoints:**\n",
    "- `GET /health` - Health check\n",
    "- `POST /train` - Trigger training (params: `target`, `timesteps`, `epochs`)\n",
    "- `GET /status` - Get training status\n",
    "\n",
    "**Workflow:**\n",
    "1. Run cells 1-6 to set up dependencies, config, and functions\n",
    "2. Run the API cell to start the server\n",
    "3. Copy the ngrok URL and configure in Airflow\n",
    "4. Airflow triggers training via API call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a717713c",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch pandas numpy scikit-learn pyarrow requests flask pyngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631fa1a",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe901e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - CLOUDFLARE TUNNEL URLs\n",
    "# ============================================\n",
    "\n",
    "# HDFS via Cloudflare Tunnels\n",
    "HDFS_NAMENODE_URL = \"https://lab-jurisdiction-arrives-sys.trycloudflare.com\"\n",
    "HDFS_DATANODE_URL = \"https://scholar-march-certification-tests.trycloudflare.com\"\n",
    "HDFS_USER = \"root\"\n",
    "\n",
    "# HDFS Paths\n",
    "HDFS_FEATURES_PATH = \"/data/processed/features.parquet\"\n",
    "HDFS_MODELS_DIR = \"/models\"\n",
    "\n",
    "# LSTM Training Configuration (defaults, can be overridden via API)\n",
    "DEFAULT_TARGET = \"heatwave\"\n",
    "DEFAULT_TIMESTEPS = 14\n",
    "DEFAULT_EPOCHS = 20\n",
    "HIDDEN_SIZE = 64\n",
    "FC_SIZE = 32\n",
    "BATCH_SIZE = 128\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# API Configuration\n",
    "API_PORT = 5001  # Different port from XGBoost API\n",
    "NGROK_AUTH_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75797666",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80484db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle\n",
    "import requests\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca185134",
   "metadata": {},
   "source": [
    "## 4. HDFS Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb301d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDFSManager:\n",
    "    \"\"\"Manages HDFS operations via WebHDFS through Cloudflare tunnels.\n",
    "    \n",
    "    Uses separate tunnels for NameNode and DataNode to handle WebHDFS redirects.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, namenode_url, datanode_url, user):\n",
    "        self.namenode_url = namenode_url.rstrip('/')\n",
    "        self.datanode_url = datanode_url.rstrip('/')\n",
    "        self.user = user\n",
    "        self.session = requests.Session()\n",
    "        print(f\"‚úÖ HDFS Manager initialized\")\n",
    "        print(f\"   NameNode: {self.namenode_url}\")\n",
    "        print(f\"   DataNode: {self.datanode_url}\")\n",
    "    \n",
    "    def _namenode_api(self, path, op, **params):\n",
    "        \"\"\"Build NameNode WebHDFS URL.\"\"\"\n",
    "        url = f\"{self.namenode_url}/webhdfs/v1{path}?op={op}&user.name={self.user}\"\n",
    "        for k, v in params.items():\n",
    "            url += f\"&{k}={v}\"\n",
    "        return url\n",
    "    \n",
    "    def _redirect_to_datanode(self, redirect_url):\n",
    "        \"\"\"Convert NameNode redirect URL to use DataNode tunnel.\"\"\"\n",
    "        parsed = urlparse(redirect_url)\n",
    "        return f\"{self.datanode_url}{parsed.path}?{parsed.query}\" if parsed.query else f\"{self.datanode_url}{parsed.path}\"\n",
    "    \n",
    "    def read_parquet(self, hdfs_path):\n",
    "        \"\"\"Read parquet file from HDFS into pandas DataFrame.\"\"\"\n",
    "        print(f\"üìñ Reading parquet from: {hdfs_path}\")\n",
    "        \n",
    "        # Step 1: Request OPEN from NameNode (returns 307 redirect to DataNode)\n",
    "        open_url = self._namenode_api(hdfs_path, \"OPEN\")\n",
    "        response = self.session.get(open_url, allow_redirects=False, timeout=60)\n",
    "        \n",
    "        if response.status_code == 307:\n",
    "            # Step 2: Get redirect and convert to DataNode tunnel URL\n",
    "            redirect_url = response.headers.get('Location')\n",
    "            datanode_url = self._redirect_to_datanode(redirect_url)\n",
    "            print(f\"   Fetching from DataNode tunnel...\")\n",
    "            \n",
    "            # Step 3: Download from DataNode tunnel\n",
    "            response = self.session.get(datanode_url, timeout=300)\n",
    "            response.raise_for_status()\n",
    "        elif response.status_code == 200:\n",
    "            pass  # Direct response\n",
    "        else:\n",
    "            raise Exception(f\"Failed to open file: {response.status_code} - {response.text[:500]}\")\n",
    "        \n",
    "        # Parse parquet\n",
    "        df = pd.read_parquet(io.BytesIO(response.content))\n",
    "        print(f\"‚úÖ Loaded DataFrame with shape: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    def save_model_torch(self, model_dict, hdfs_path):\n",
    "        \"\"\"Save PyTorch model state dict to HDFS.\"\"\"\n",
    "        print(f\"üíæ Saving model to: {hdfs_path}\")\n",
    "        \n",
    "        # Serialize model using torch.save to bytes\n",
    "        buffer = io.BytesIO()\n",
    "        torch.save(model_dict, buffer)\n",
    "        model_bytes = buffer.getvalue()\n",
    "        \n",
    "        # Step 1: CREATE request to NameNode (returns 307 redirect)\n",
    "        create_url = self._namenode_api(hdfs_path, \"CREATE\", overwrite=\"true\")\n",
    "        response = self.session.put(create_url, allow_redirects=False, timeout=60)\n",
    "        \n",
    "        if response.status_code == 307:\n",
    "            # Step 2: Upload to DataNode tunnel\n",
    "            redirect_url = response.headers.get('Location')\n",
    "            datanode_url = self._redirect_to_datanode(redirect_url)\n",
    "            \n",
    "            response = self.session.put(\n",
    "                datanode_url,\n",
    "                data=model_bytes,\n",
    "                headers={'Content-Type': 'application/octet-stream'},\n",
    "                timeout=300\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "        else:\n",
    "            raise Exception(f\"Unexpected response: {response.status_code} - {response.text[:500]}\")\n",
    "        \n",
    "        print(f\"‚úÖ Model saved! Size: {len(model_bytes) / 1024:.2f} KB\")\n",
    "        return hdfs_path\n",
    "    \n",
    "    def load_model_torch(self, hdfs_path):\n",
    "        \"\"\"Load PyTorch model from HDFS.\"\"\"\n",
    "        print(f\"üìñ Loading model from: {hdfs_path}\")\n",
    "        \n",
    "        open_url = self._namenode_api(hdfs_path, \"OPEN\")\n",
    "        response = self.session.get(open_url, allow_redirects=False, timeout=60)\n",
    "        \n",
    "        if response.status_code == 307:\n",
    "            redirect_url = response.headers.get('Location')\n",
    "            datanode_url = self._redirect_to_datanode(redirect_url)\n",
    "            response = self.session.get(datanode_url, timeout=300)\n",
    "            response.raise_for_status()\n",
    "        \n",
    "        model_dict = torch.load(io.BytesIO(response.content), map_location='cpu')\n",
    "        print(f\"‚úÖ Model loaded!\")\n",
    "        return model_dict\n",
    "    \n",
    "    def list_files(self, hdfs_path):\n",
    "        \"\"\"List files in HDFS directory.\"\"\"\n",
    "        try:\n",
    "            url = self._namenode_api(hdfs_path, \"LISTSTATUS\")\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            files = [f['pathSuffix'] for f in data.get('FileStatuses', {}).get('FileStatus', [])]\n",
    "            return files\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing {hdfs_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def mkdir(self, hdfs_path):\n",
    "        \"\"\"Create directory in HDFS.\"\"\"\n",
    "        url = self._namenode_api(hdfs_path, \"MKDIRS\")\n",
    "        response = self.session.put(url, timeout=30)\n",
    "        return response.json().get('boolean', False)\n",
    "\n",
    "# Initialize HDFS Manager with both tunnel URLs\n",
    "hdfs_manager = HDFSManager(HDFS_NAMENODE_URL, HDFS_DATANODE_URL, HDFS_USER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c14fe0",
   "metadata": {},
   "source": [
    "## 5. LSTM Model & Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"LSTM model for binary classification (matching train_lstm.py).\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=64, fc_size=32):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, fc_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        out = self.fc1(h_n[-1])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "\n",
    "def build_sequences(df, timesteps=14, target='heatwave'):\n",
    "    \"\"\"Build sliding window sequences (matching train_lstm.py).\"\"\"\n",
    "    df = df.sort_values(['District', 'Date']).copy()\n",
    "    exclude_cols = ['Date', 'District', 'heatwave', 'flood_proxy']\n",
    "    feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "    \n",
    "    sequences, targets = [], []\n",
    "    for district, group in df.groupby('District'):\n",
    "        arr = group[feature_cols].values\n",
    "        lab = group[target].fillna(False).astype(int).values\n",
    "        if len(arr) < timesteps + 1:\n",
    "            continue\n",
    "        for i in range(timesteps, len(arr)):\n",
    "            sequences.append(arr[i-timesteps:i])\n",
    "            targets.append(lab[i])\n",
    "    \n",
    "    X = np.array(sequences, dtype=np.float32)\n",
    "    y = np.array(targets, dtype=np.float32)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "\n",
    "def train_lstm_model(hdfs_manager, target='heatwave', timesteps=14, epochs=20):\n",
    "    \"\"\"Full LSTM training pipeline (matching train_lstm.py logic).\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéØ TRAINING LSTM FOR: {target.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"üì• Loading data from HDFS...\")\n",
    "    df = hdfs_manager.read_parquet(HDFS_FEATURES_PATH)\n",
    "    \n",
    "    # Build sequences\n",
    "    X, y, feature_cols = build_sequences(df, timesteps=timesteps, target=target)\n",
    "    print(f\"üìä Sequences: {X.shape}, Positive: {y.sum():.0f} ({y.mean()*100:.2f}%)\")\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, shuffle=True, random_state=42\n",
    "    )\n",
    "    \n",
    "    n_samples, n_timesteps, n_features = X_train.shape\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.reshape(-1, n_features)).reshape(X_train.shape)\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(torch.FloatTensor(X_train_scaled), torch.FloatTensor(y_train)),\n",
    "        batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        TensorDataset(torch.FloatTensor(X_test_scaled), torch.FloatTensor(y_test)),\n",
    "        batch_size=BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    model = LSTMModel(n_features, HIDDEN_SIZE, FC_SIZE).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    # Training\n",
    "    print(\"üöÄ Training model...\")\n",
    "    best_auc = 0.0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X_batch), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                probs = torch.sigmoid(model(X_batch))\n",
    "                all_preds.extend(probs.cpu().numpy())\n",
    "                all_targets.extend(y_batch.numpy())\n",
    "        \n",
    "        auc = roc_auc_score(all_targets, all_preds) if len(set(all_targets)) > 1 else 0\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_state = model.state_dict().copy()\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"   Epoch {epoch+1}/{epochs} - AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Load best\n",
    "    model.load_state_dict(best_state)\n",
    "    \n",
    "    # Final metrics\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            probs = torch.sigmoid(model(X_batch.to(device)))\n",
    "            all_preds.extend(probs.cpu().numpy())\n",
    "            all_targets.extend(y_batch.numpy())\n",
    "    \n",
    "    val_auc = roc_auc_score(all_targets, all_preds)\n",
    "    print(f\"\\n‚úÖ Best Val AUC: {best_auc:.4f}\")\n",
    "    \n",
    "    metrics = {'val_auc': float(val_auc), 'best_auc': float(best_auc)}\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'input_size': n_features,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'fc_size': FC_SIZE,\n",
    "        'timesteps': timesteps,\n",
    "        'feature_cols': feature_cols,\n",
    "        'scaler_mean': scaler.mean_.tolist(),\n",
    "        'scaler_scale': scaler.scale_.tolist(),\n",
    "        'target': target,\n",
    "        'metrics': metrics,\n",
    "        'saved_at': datetime.now().isoformat(),\n",
    "        'pytorch_version': torch.__version__\n",
    "    }\n",
    "    \n",
    "    hdfs_manager.mkdir(HDFS_MODELS_DIR)\n",
    "    hdfs_path = f\"{HDFS_MODELS_DIR}/lstm_{target}.pt\"\n",
    "    hdfs_manager.save_model_torch(checkpoint, hdfs_path)\n",
    "    print(f\"üíæ Saved to: {hdfs_path}\")\n",
    "    \n",
    "    return {'model': model, 'metrics': metrics, 'hdfs_path': hdfs_path}\n",
    "\n",
    "print(\"‚úÖ LSTM model and training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9791d08",
   "metadata": {},
   "source": [
    "## 6. Flask API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global state for tracking training\n",
    "training_state = {\n",
    "    'is_training': False,\n",
    "    'current_target': None,\n",
    "    'last_result': None,\n",
    "    'last_error': None,\n",
    "    'started_at': None,\n",
    "    'completed_at': None\n",
    "}\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize HDFS Manager\n",
    "hdfs_manager = HDFSManager(HDFS_NAMENODE_URL, HDFS_DATANODE_URL, HDFS_USER)\n",
    "\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'service': 'lstm-training-api',\n",
    "        'device': str(device),\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "\n",
    "@app.route('/status', methods=['GET'])\n",
    "def status():\n",
    "    \"\"\"Get current training status.\"\"\"\n",
    "    return jsonify({\n",
    "        'is_training': training_state['is_training'],\n",
    "        'current_target': training_state['current_target'],\n",
    "        'started_at': training_state['started_at'],\n",
    "        'completed_at': training_state['completed_at'],\n",
    "        'last_result': training_state['last_result'],\n",
    "        'last_error': training_state['last_error']\n",
    "    })\n",
    "\n",
    "\n",
    "def run_training(target, timesteps, epochs):\n",
    "    \"\"\"Background training function.\"\"\"\n",
    "    global training_state\n",
    "    \n",
    "    try:\n",
    "        training_state['is_training'] = True\n",
    "        training_state['current_target'] = target\n",
    "        training_state['started_at'] = datetime.now().isoformat()\n",
    "        training_state['last_error'] = None\n",
    "        \n",
    "        result = train_lstm_model(hdfs_manager, target=target, timesteps=timesteps, epochs=epochs)\n",
    "        \n",
    "        training_state['last_result'] = {\n",
    "            'target': target,\n",
    "            'metrics': result['metrics'],\n",
    "            'hdfs_path': result['hdfs_path']\n",
    "        }\n",
    "        training_state['completed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "    except Exception as e:\n",
    "        training_state['last_error'] = str(e)\n",
    "        print(f\"‚ùå Training error: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        training_state['is_training'] = False\n",
    "        training_state['current_target'] = None\n",
    "\n",
    "\n",
    "@app.route('/train', methods=['POST'])\n",
    "def train():\n",
    "    \"\"\"Trigger model training.\"\"\"\n",
    "    if training_state['is_training']:\n",
    "        return jsonify({\n",
    "            'status': 'busy',\n",
    "            'message': f\"Training already in progress for {training_state['current_target']}\"\n",
    "        }), 409\n",
    "    \n",
    "    data = request.get_json() or {}\n",
    "    target = data.get('target', DEFAULT_TARGET)\n",
    "    timesteps = data.get('timesteps', DEFAULT_TIMESTEPS)\n",
    "    epochs = data.get('epochs', DEFAULT_EPOCHS)\n",
    "    async_mode = data.get('async', False)\n",
    "    \n",
    "    if target not in ['heatwave', 'flood_proxy']:\n",
    "        return jsonify({'status': 'error', 'message': f\"Invalid target: {target}\"}), 400\n",
    "    \n",
    "    if async_mode:\n",
    "        thread = threading.Thread(target=run_training, args=(target, timesteps, epochs))\n",
    "        thread.start()\n",
    "        return jsonify({\n",
    "            'status': 'started',\n",
    "            'message': f'Training started for {target}',\n",
    "            'target': target\n",
    "        })\n",
    "    else:\n",
    "        run_training(target, timesteps, epochs)\n",
    "        \n",
    "        if training_state['last_error']:\n",
    "            return jsonify({'status': 'error', 'message': training_state['last_error']}), 500\n",
    "        \n",
    "        return jsonify({'status': 'completed', 'result': training_state['last_result']})\n",
    "\n",
    "\n",
    "@app.route('/train/all', methods=['POST'])\n",
    "def train_all():\n",
    "    \"\"\"Train models for all targets.\"\"\"\n",
    "    if training_state['is_training']:\n",
    "        return jsonify({'status': 'busy'}), 409\n",
    "    \n",
    "    data = request.get_json() or {}\n",
    "    timesteps = data.get('timesteps', DEFAULT_TIMESTEPS)\n",
    "    epochs = data.get('epochs', DEFAULT_EPOCHS)\n",
    "    \n",
    "    results = {}\n",
    "    for target in ['heatwave', 'flood_proxy']:\n",
    "        run_training(target, timesteps, epochs)\n",
    "        if training_state['last_error']:\n",
    "            results[target] = {'status': 'error', 'error': training_state['last_error']}\n",
    "        else:\n",
    "            results[target] = {'status': 'completed', 'result': training_state['last_result']}\n",
    "    \n",
    "    return jsonify({'status': 'completed', 'results': results})\n",
    "\n",
    "\n",
    "print(\"‚úÖ Flask API configured!\")\n",
    "print(\"   Endpoints: /health, /status, /train, /train/all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab474a",
   "metadata": {},
   "source": [
    "## 7. Start API Server with ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4575f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ngrok auth token if provided\n",
    "if NGROK_AUTH_TOKEN:\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(API_PORT)\n",
    "print(\"=\" * 60)\n",
    "print(\"üåê LSTM TRAINING API SERVER\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüì° Public URL: {public_url}\")\n",
    "print(f\"   Local URL:  http://localhost:{API_PORT}\")\n",
    "print(\"\\nüìã API Endpoints:\")\n",
    "print(f\"   GET  {public_url}/health\")\n",
    "print(f\"   GET  {public_url}/status\")\n",
    "print(f\"   POST {public_url}/train\")\n",
    "print(f\"   POST {public_url}/train/all\")\n",
    "print(\"\\n‚ö†Ô∏è  Copy the public URL and configure in Airflow DAG!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "LSTM_API_URL = str(public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d014b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Flask server (this cell blocks - run this last!)\n",
    "print(\"üöÄ Starting Flask server...\")\n",
    "print(\"   Press Ctrl+C or restart runtime to stop\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "app.run(port=API_PORT, threaded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2ba69",
   "metadata": {},
   "source": [
    "## 8. API Usage Examples\n",
    "\n",
    "### From Airflow (Python):\n",
    "```python\n",
    "import requests\n",
    "\n",
    "LSTM_API_URL = \"https://xxxx.ngrok.io\"  # Replace with actual ngrok URL\n",
    "\n",
    "# Train heatwave model\n",
    "response = requests.post(f\"{LSTM_API_URL}/train\", json={\n",
    "    \"target\": \"heatwave\",\n",
    "    \"timesteps\": 14,\n",
    "    \"epochs\": 20\n",
    "})\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "### From curl:\n",
    "```bash\n",
    "curl -X POST https://xxxx.ngrok.io/train \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"target\": \"heatwave\", \"timesteps\": 14, \"epochs\": 20}'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
