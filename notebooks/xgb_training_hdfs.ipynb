{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e58ffbb",
   "metadata": {},
   "source": [
    "# üåä XGBoost Training API Notebook - Heatwave/Flood Prediction\n",
    "\n",
    "This notebook exposes XGBoost model training as a **REST API** that can be triggered by Airflow or other orchestration tools.\n",
    "\n",
    "**Features:**\n",
    "- Flask API with `/train` endpoint for triggering training\n",
    "- Public URL via ngrok for remote access from Airflow\n",
    "- Loads training data from HDFS via Cloudflare tunnels\n",
    "- Saves trained models back to HDFS\n",
    "\n",
    "**API Endpoints:**\n",
    "- `GET /health` - Health check\n",
    "- `POST /train` - Trigger training (params: `target`, `test_days`)\n",
    "- `GET /status` - Get training status\n",
    "\n",
    "**Workflow:**\n",
    "1. Run cells 1-7 to set up dependencies, config, and functions\n",
    "2. Run the API cell to start the server\n",
    "3. Copy the ngrok URL and configure in Airflow\n",
    "4. Airflow triggers training via API call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94651e37",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q xgboost pandas numpy scikit-learn pyarrow requests joblib matplotlib seaborn flask pyngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629d8e2",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - CLOUDFLARE TUNNEL URLs\n",
    "# ============================================\n",
    "\n",
    "# HDFS via Cloudflare Tunnels\n",
    "# Get URLs from: docker logs cloudflared-hdfs-namenode / cloudflared-hdfs-datanode\n",
    "HDFS_NAMENODE_URL = \"https://lab-jurisdiction-arrives-sys.trycloudflare.com\"      # NameNode tunnel\n",
    "HDFS_DATANODE_URL = \"https://scholar-march-certification-tests.trycloudflare.com\" # DataNode tunnel\n",
    "HDFS_USER = \"root\"\n",
    "\n",
    "# HDFS Paths\n",
    "HDFS_FEATURES_PATH = \"/data/processed/features.parquet\"  # Input features parquet\n",
    "HDFS_MODELS_DIR = \"/models\"                               # Output directory for models\n",
    "\n",
    "# Training Configuration (defaults, can be overridden via API)\n",
    "DEFAULT_TARGET = \"heatwave\"        # Options: \"heatwave\" or \"flood_proxy\"\n",
    "DEFAULT_TEST_DAYS = 365            # Number of days for test split\n",
    "\n",
    "# XGBoost Parameters (matching train_xgb.py)\n",
    "XGB_PARAMS = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'aucpr',\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "}\n",
    "NUM_BOOST_ROUNDS = 1000\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "# API Configuration\n",
    "API_PORT = 5000\n",
    "NGROK_AUTH_TOKEN = \"\"  # Optional: Set your ngrok auth token for stable URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728a55c",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73893874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle\n",
    "import joblib\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, \n",
    "    roc_auc_score, \n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from pyngrok import ngrok\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"   XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696056d3",
   "metadata": {},
   "source": [
    "## 4. HDFS Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca02e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDFSManager:\n",
    "    \"\"\"Manages HDFS operations via WebHDFS through Cloudflare tunnels.\n",
    "    \n",
    "    Uses separate tunnels for NameNode and DataNode to handle WebHDFS redirects.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, namenode_url, datanode_url, user):\n",
    "        self.namenode_url = namenode_url.rstrip('/')\n",
    "        self.datanode_url = datanode_url.rstrip('/')\n",
    "        self.user = user\n",
    "        self.session = requests.Session()\n",
    "        print(f\"‚úÖ HDFS Manager initialized\")\n",
    "        print(f\"   NameNode: {self.namenode_url}\")\n",
    "        print(f\"   DataNode: {self.datanode_url}\")\n",
    "    \n",
    "    def _namenode_api(self, path, op, **params):\n",
    "        \"\"\"Build NameNode WebHDFS URL.\"\"\"\n",
    "        url = f\"{self.namenode_url}/webhdfs/v1{path}?op={op}&user.name={self.user}\"\n",
    "        for k, v in params.items():\n",
    "            url += f\"&{k}={v}\"\n",
    "        return url\n",
    "    \n",
    "    def _redirect_to_datanode(self, redirect_url):\n",
    "        \"\"\"Convert NameNode redirect URL to use DataNode tunnel.\"\"\"\n",
    "        parsed = urlparse(redirect_url)\n",
    "        return f\"{self.datanode_url}{parsed.path}?{parsed.query}\" if parsed.query else f\"{self.datanode_url}{parsed.path}\"\n",
    "    \n",
    "    def read_parquet(self, hdfs_path):\n",
    "        \"\"\"Read parquet file from HDFS into pandas DataFrame.\"\"\"\n",
    "        print(f\"üìñ Reading parquet from: {hdfs_path}\")\n",
    "        \n",
    "        # Step 1: Request OPEN from NameNode (returns 307 redirect to DataNode)\n",
    "        open_url = self._namenode_api(hdfs_path, \"OPEN\")\n",
    "        response = self.session.get(open_url, allow_redirects=False, timeout=60)\n",
    "        \n",
    "        if response.status_code == 307:\n",
    "            # Step 2: Get redirect and convert to DataNode tunnel URL\n",
    "            redirect_url = response.headers.get('Location')\n",
    "            datanode_url = self._redirect_to_datanode(redirect_url)\n",
    "            print(f\"   Fetching from DataNode tunnel...\")\n",
    "            \n",
    "            # Step 3: Download from DataNode tunnel\n",
    "            response = self.session.get(datanode_url, timeout=300)\n",
    "            response.raise_for_status()\n",
    "        elif response.status_code == 200:\n",
    "            pass  # Direct response\n",
    "        else:\n",
    "            raise Exception(f\"Failed to open file: {response.status_code} - {response.text[:500]}\")\n",
    "        \n",
    "        # Parse parquet\n",
    "        df = pd.read_parquet(io.BytesIO(response.content))\n",
    "        print(f\"‚úÖ Loaded DataFrame with shape: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    def save_model_joblib(self, model, hdfs_path, metadata=None):\n",
    "        \"\"\"Save model using joblib to HDFS (matching train_xgb.py format).\"\"\"\n",
    "        print(f\"üíæ Saving model to: {hdfs_path}\")\n",
    "        \n",
    "        # Create model package with metadata\n",
    "        model_package = {\n",
    "            'model': model,\n",
    "            'metadata': metadata or {},\n",
    "        }\n",
    "        \n",
    "        # Serialize using joblib (same as train_xgb.py)\n",
    "        buffer = io.BytesIO()\n",
    "        joblib.dump(model_package, buffer)\n",
    "        model_bytes = buffer.getvalue()\n",
    "        \n",
    "        # Step 1: CREATE request to NameNode (returns 307 redirect)\n",
    "        create_url = self._namenode_api(hdfs_path, \"CREATE\", overwrite=\"true\")\n",
    "        response = self.session.put(create_url, allow_redirects=False, timeout=60)\n",
    "        \n",
    "        if response.status_code == 307:\n",
    "            # Step 2: Upload to DataNode tunnel\n",
    "            redirect_url = response.headers.get('Location')\n",
    "            datanode_url = self._redirect_to_datanode(redirect_url)\n",
    "            \n",
    "            response = self.session.put(\n",
    "                datanode_url,\n",
    "                data=model_bytes,\n",
    "                headers={'Content-Type': 'application/octet-stream'},\n",
    "                timeout=300\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "        else:\n",
    "            raise Exception(f\"Unexpected response: {response.status_code} - {response.text[:500]}\")\n",
    "        \n",
    "        print(f\"‚úÖ Model saved! Size: {len(model_bytes) / 1024:.2f} KB\")\n",
    "        return hdfs_path\n",
    "    \n",
    "    def load_model_joblib(self, hdfs_path):\n",
    "        \"\"\"Load joblib model from HDFS.\"\"\"\n",
    "        print(f\"üìñ Loading model from: {hdfs_path}\")\n",
    "        \n",
    "        open_url = self._namenode_api(hdfs_path, \"OPEN\")\n",
    "        response = self.session.get(open_url, allow_redirects=False, timeout=60)\n",
    "        \n",
    "        if response.status_code == 307:\n",
    "            redirect_url = response.headers.get('Location')\n",
    "            datanode_url = self._redirect_to_datanode(redirect_url)\n",
    "            response = self.session.get(datanode_url, timeout=300)\n",
    "            response.raise_for_status()\n",
    "        \n",
    "        model_package = joblib.load(io.BytesIO(response.content))\n",
    "        print(f\"‚úÖ Model loaded!\")\n",
    "        return model_package\n",
    "    \n",
    "    def list_files(self, hdfs_path):\n",
    "        \"\"\"List files in HDFS directory.\"\"\"\n",
    "        try:\n",
    "            url = self._namenode_api(hdfs_path, \"LISTSTATUS\")\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            files = [f['pathSuffix'] for f in data.get('FileStatuses', {}).get('FileStatus', [])]\n",
    "            return files\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing {hdfs_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def mkdir(self, hdfs_path):\n",
    "        \"\"\"Create directory in HDFS.\"\"\"\n",
    "        url = self._namenode_api(hdfs_path, \"MKDIRS\")\n",
    "        response = self.session.put(url, timeout=30)\n",
    "        return response.json().get('boolean', False)\n",
    "\n",
    "# Initialize HDFS Manager with both tunnel URLs\n",
    "hdfs_manager = HDFSManager(HDFS_NAMENODE_URL, HDFS_DATANODE_URL, HDFS_USER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddda6fd",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f55d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, target='heatwave'):\n",
    "    \"\"\"Prepare features and target for training (matching train_xgb.py).\"\"\"\n",
    "    df = df.sort_values('Date').copy()\n",
    "    df = df[~df[target].isna()].copy()\n",
    "    \n",
    "    drop_cols = ['Date', 'District', 'heatwave', 'flood_proxy', 'Latitude', 'Longitude']\n",
    "    feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target].astype(int)\n",
    "    X = X.fillna(-999)\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "\n",
    "def time_train_test_split(df, test_size_days=365):\n",
    "    \"\"\"Split data by time - last N days as test (matching train_xgb.py).\"\"\"\n",
    "    df = df.sort_values('Date').copy()\n",
    "    max_date = df['Date'].max()\n",
    "    cutoff = max_date - pd.Timedelta(days=test_size_days)\n",
    "    \n",
    "    train = df[df['Date'] < cutoff]\n",
    "    test = df[df['Date'] >= cutoff]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def train_xgb_model(hdfs_manager, target='heatwave', test_days=365):\n",
    "    \"\"\"Full XGBoost training pipeline (matching train_xgb.py logic).\n",
    "    \n",
    "    Args:\n",
    "        hdfs_manager: HDFSManager instance\n",
    "        target: 'heatwave' or 'flood_proxy'\n",
    "        test_days: Number of days for test set\n",
    "    \n",
    "    Returns:\n",
    "        dict with model, metrics, and metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéØ TRAINING XGBoost FOR: {target.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load data from HDFS\n",
    "    print(\"üì• Loading data from HDFS...\")\n",
    "    df = hdfs_manager.read_parquet(HDFS_FEATURES_PATH)\n",
    "    \n",
    "    # Time-based split\n",
    "    train_df, test_df = time_train_test_split(df, test_size_days=test_days)\n",
    "    print(f\"üìÖ Train: {len(train_df)} samples, Test: {len(test_df)} samples\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, y_train, feature_cols = prepare_data(train_df, target=target)\n",
    "    X_test, y_test, _ = prepare_data(test_df, target=target)\n",
    "    \n",
    "    # Create DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_cols)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test, feature_names=feature_cols)\n",
    "    \n",
    "    # Calculate scale_pos_weight (matching train_xgb.py)\n",
    "    scale_pos_weight = max(1, (len(y_train) - y_train.sum()) / max(1, y_train.sum()))\n",
    "    \n",
    "    # Parameters\n",
    "    params = XGB_PARAMS.copy()\n",
    "    params['scale_pos_weight'] = scale_pos_weight\n",
    "    \n",
    "    # Train\n",
    "    print(\"üöÄ Training model...\")\n",
    "    evals = [(dtrain, 'train'), (dtest, 'test')]\n",
    "    model = xgb.train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=NUM_BOOST_ROUNDS,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "        verbose_eval=50\n",
    "    )\n",
    "    \n",
    "    # Evaluate (matching train_xgb.py output)\n",
    "    y_pred_proba = model.predict(dtest)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nPR AUC: {pr_auc:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Precision {prec:.3f}, Recall {rec:.3f}, F1 {f1:.3f}\")\n",
    "    \n",
    "    metrics = {\n",
    "        'pr_auc': float(pr_auc),\n",
    "        'roc_auc': float(roc_auc),\n",
    "        'precision': float(prec),\n",
    "        'recall': float(rec),\n",
    "        'f1': float(f1)\n",
    "    }\n",
    "    \n",
    "    metadata = {\n",
    "        'target': target,\n",
    "        'feature_columns': feature_cols,\n",
    "        'training_params': params,\n",
    "        'best_iteration': model.best_iteration,\n",
    "        'metrics': metrics,\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'saved_at': datetime.now().isoformat(),\n",
    "        'xgboost_version': xgb.__version__\n",
    "    }\n",
    "    \n",
    "    # Save to HDFS\n",
    "    hdfs_manager.mkdir(HDFS_MODELS_DIR)\n",
    "    hdfs_path = f\"{HDFS_MODELS_DIR}/xgb_{target}_model.joblib\"\n",
    "    hdfs_manager.save_model_joblib(model, hdfs_path, metadata=metadata)\n",
    "    print(f\"üíæ Saved to: {hdfs_path}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': metrics,\n",
    "        'metadata': metadata,\n",
    "        'hdfs_path': hdfs_path\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df24e12",
   "metadata": {},
   "source": [
    "## 6. Flask API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global state for tracking training\n",
    "training_state = {\n",
    "    'is_training': False,\n",
    "    'current_target': None,\n",
    "    'last_result': None,\n",
    "    'last_error': None,\n",
    "    'started_at': None,\n",
    "    'completed_at': None\n",
    "}\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize HDFS Manager\n",
    "hdfs_manager = HDFSManager(HDFS_NAMENODE_URL, HDFS_DATANODE_URL, HDFS_USER)\n",
    "\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'service': 'xgb-training-api',\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "\n",
    "@app.route('/status', methods=['GET'])\n",
    "def status():\n",
    "    \"\"\"Get current training status.\"\"\"\n",
    "    return jsonify({\n",
    "        'is_training': training_state['is_training'],\n",
    "        'current_target': training_state['current_target'],\n",
    "        'started_at': training_state['started_at'],\n",
    "        'completed_at': training_state['completed_at'],\n",
    "        'last_result': training_state['last_result'],\n",
    "        'last_error': training_state['last_error']\n",
    "    })\n",
    "\n",
    "\n",
    "def run_training(target, test_days):\n",
    "    \"\"\"Background training function.\"\"\"\n",
    "    global training_state\n",
    "    \n",
    "    try:\n",
    "        training_state['is_training'] = True\n",
    "        training_state['current_target'] = target\n",
    "        training_state['started_at'] = datetime.now().isoformat()\n",
    "        training_state['last_error'] = None\n",
    "        \n",
    "        # Run training\n",
    "        result = train_xgb_model(hdfs_manager, target=target, test_days=test_days)\n",
    "        \n",
    "        training_state['last_result'] = {\n",
    "            'target': target,\n",
    "            'metrics': result['metrics'],\n",
    "            'hdfs_path': result['hdfs_path'],\n",
    "            'best_iteration': result['metadata']['best_iteration']\n",
    "        }\n",
    "        training_state['completed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "    except Exception as e:\n",
    "        training_state['last_error'] = str(e)\n",
    "        print(f\"‚ùå Training error: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        training_state['is_training'] = False\n",
    "        training_state['current_target'] = None\n",
    "\n",
    "\n",
    "@app.route('/train', methods=['POST'])\n",
    "def train():\n",
    "    \"\"\"Trigger model training.\n",
    "    \n",
    "    Request body (JSON):\n",
    "        - target: 'heatwave' or 'flood_proxy' (default: 'heatwave')\n",
    "        - test_days: Number of days for test set (default: 365)\n",
    "        - async: If true, return immediately (default: false)\n",
    "    \"\"\"\n",
    "    if training_state['is_training']:\n",
    "        return jsonify({\n",
    "            'status': 'busy',\n",
    "            'message': f\"Training already in progress for {training_state['current_target']}\",\n",
    "            'started_at': training_state['started_at']\n",
    "        }), 409\n",
    "    \n",
    "    # Parse request\n",
    "    data = request.get_json() or {}\n",
    "    target = data.get('target', DEFAULT_TARGET)\n",
    "    test_days = data.get('test_days', DEFAULT_TEST_DAYS)\n",
    "    async_mode = data.get('async', False)\n",
    "    \n",
    "    # Validate target\n",
    "    if target not in ['heatwave', 'flood_proxy']:\n",
    "        return jsonify({\n",
    "            'status': 'error',\n",
    "            'message': f\"Invalid target: {target}. Must be 'heatwave' or 'flood_proxy'\"\n",
    "        }), 400\n",
    "    \n",
    "    if async_mode:\n",
    "        # Start training in background thread\n",
    "        thread = threading.Thread(target=run_training, args=(target, test_days))\n",
    "        thread.start()\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': 'started',\n",
    "            'message': f'Training started for {target}',\n",
    "            'target': target,\n",
    "            'test_days': test_days,\n",
    "            'started_at': datetime.now().isoformat()\n",
    "        })\n",
    "    else:\n",
    "        # Synchronous training\n",
    "        run_training(target, test_days)\n",
    "        \n",
    "        if training_state['last_error']:\n",
    "            return jsonify({\n",
    "                'status': 'error',\n",
    "                'message': training_state['last_error']\n",
    "            }), 500\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': 'completed',\n",
    "            'result': training_state['last_result']\n",
    "        })\n",
    "\n",
    "\n",
    "@app.route('/train/all', methods=['POST'])\n",
    "def train_all():\n",
    "    \"\"\"Train models for all targets (heatwave and flood_proxy).\"\"\"\n",
    "    if training_state['is_training']:\n",
    "        return jsonify({\n",
    "            'status': 'busy',\n",
    "            'message': f\"Training already in progress for {training_state['current_target']}\"\n",
    "        }), 409\n",
    "    \n",
    "    data = request.get_json() or {}\n",
    "    test_days = data.get('test_days', DEFAULT_TEST_DAYS)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for target in ['heatwave', 'flood_proxy']:\n",
    "        run_training(target, test_days)\n",
    "        \n",
    "        if training_state['last_error']:\n",
    "            results[target] = {'status': 'error', 'error': training_state['last_error']}\n",
    "        else:\n",
    "            results[target] = {'status': 'completed', 'result': training_state['last_result']}\n",
    "    \n",
    "    return jsonify({\n",
    "        'status': 'completed',\n",
    "        'results': results\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"‚úÖ Flask API configured!\")\n",
    "print(\"   Endpoints:\")\n",
    "print(\"   - GET  /health  - Health check\")\n",
    "print(\"   - GET  /status  - Training status\")\n",
    "print(\"   - POST /train   - Train single model\")\n",
    "print(\"   - POST /train/all - Train all models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Start API Server with ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ngrok auth token if provided\n",
    "if NGROK_AUTH_TOKEN:\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(API_PORT)\n",
    "print(\"=\" * 60)\n",
    "print(\"üåê XGBoost TRAINING API SERVER\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüì° Public URL: {public_url}\")\n",
    "print(f\"   Local URL:  http://localhost:{API_PORT}\")\n",
    "print(\"\\nüìã API Endpoints:\")\n",
    "print(f\"   GET  {public_url}/health\")\n",
    "print(f\"   GET  {public_url}/status\")\n",
    "print(f\"   POST {public_url}/train\")\n",
    "print(f\"   POST {public_url}/train/all\")\n",
    "print(\"\\n‚ö†Ô∏è  Copy the public URL and configure in Airflow DAG!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store URL for reference\n",
    "XGB_API_URL = str(public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b5dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Flask server (this cell blocks - run this last!)\n",
    "print(\"üöÄ Starting Flask server...\")\n",
    "print(\"   Press Ctrl+C or restart runtime to stop\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Run Flask app\n",
    "app.run(port=API_PORT, threaded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0616ef",
   "metadata": {},
   "source": [
    "## 8. API Usage Examples\n",
    "\n",
    "### From Airflow (Python):\n",
    "```python\n",
    "import requests\n",
    "\n",
    "XGB_API_URL = \"https://xxxx.ngrok.io\"  # Replace with actual ngrok URL\n",
    "\n",
    "# Train heatwave model\n",
    "response = requests.post(f\"{XGB_API_URL}/train\", json={\n",
    "    \"target\": \"heatwave\",\n",
    "    \"test_days\": 365\n",
    "})\n",
    "print(response.json())\n",
    "\n",
    "# Train all models\n",
    "response = requests.post(f\"{XGB_API_URL}/train/all\")\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "### From curl:\n",
    "```bash\n",
    "# Health check\n",
    "curl https://xxxx.ngrok.io/health\n",
    "\n",
    "# Train heatwave model\n",
    "curl -X POST https://xxxx.ngrok.io/train \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"target\": \"heatwave\", \"test_days\": 365}'\n",
    "\n",
    "# Train all models\n",
    "curl -X POST https://xxxx.ngrok.io/train/all\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
