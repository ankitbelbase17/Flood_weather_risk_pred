{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d138777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052997e2",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3308bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Spark cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-spark-1:7077\") \\\n",
    "    .appName(\"Inference_Analysis\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d4e67b",
   "metadata": {},
   "source": [
    "## 2. Define LSTM Model Architecture\n",
    "\n",
    "Must match the architecture used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3931f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, fc_size=32, num_layers=2, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, fc_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(fc_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Use last hidden state\n",
    "        out = h_n[-1]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f03fb",
   "metadata": {},
   "source": [
    "## 3. Load Labeled Data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5863a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled data\n",
    "labeled_df = spark.read.parquet(\"hdfs://namenode:9000/user/airflow/weather_data/labeled\")\n",
    "print(f\"Total records: {labeled_df.count()}\")\n",
    "print(f\"Columns: {labeled_df.columns}\")\n",
    "\n",
    "# Show schema\n",
    "labeled_df.printSchema()\n",
    "\n",
    "# Show sample\n",
    "labeled_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31e0d1",
   "metadata": {},
   "source": [
    "## 4. Prepare Test Data\n",
    "\n",
    "Use the most recent 20% of data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fc057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for easier manipulation\n",
    "df = labeled_df.toPandas()\n",
    "\n",
    "# Sort by date and district\n",
    "df = df.sort_values(['district', 'date'])\n",
    "\n",
    "# Split by date (80% train, 20% test)\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df.iloc[:train_size]\n",
    "test_df = df.iloc[train_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test size: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTest date range: {test_df['date'].min()} to {test_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c7240",
   "metadata": {},
   "source": [
    "## 5. Create Sequences for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, target_col, sequence_length=30):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM prediction\n",
    "    \"\"\"\n",
    "    # Select feature columns (exclude date, district, and target columns)\n",
    "    exclude_cols = ['date', 'district', 'heatwave_label', 'flood_label']\n",
    "    feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    # Group by district\n",
    "    for district in data['district'].unique():\n",
    "        district_data = data[data['district'] == district].sort_values('date')\n",
    "        \n",
    "        features = district_data[feature_cols].values\n",
    "        targets = district_data[target_col].values\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(len(features) - sequence_length):\n",
    "            X.append(features[i:i+sequence_length])\n",
    "            y.append(targets[i+sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y), feature_cols\n",
    "\n",
    "# Create test sequences for heatwave prediction\n",
    "X_test_heat, y_test_heat, feature_cols = create_sequences(test_df, 'heatwave_label', sequence_length=30)\n",
    "\n",
    "# Create test sequences for flood prediction\n",
    "X_test_flood, y_test_flood, _ = create_sequences(test_df, 'flood_label', sequence_length=30)\n",
    "\n",
    "print(f\"Test sequences shape: {X_test_heat.shape}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"\\nHeatwave labels - 0: {(y_test_heat==0).sum()}, 1: {(y_test_heat==1).sum()}\")\n",
    "print(f\"Flood labels - 0: {(y_test_flood==0).sum()}, 1: {(y_test_flood==1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e921b5",
   "metadata": {},
   "source": [
    "## 6. Load Trained Models from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if models exist in HDFS\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['docker', 'exec', 'namenode', 'hdfs', 'dfs', '-ls', '/user/airflow/models/'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "print(\"Models in HDFS:\")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec6f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models from HDFS\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Create local models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Download heatwave model\n",
    "subprocess.run([\n",
    "    'docker', 'exec', 'namenode', 'hdfs', 'dfs', '-get',\n",
    "    '/user/airflow/models/heatwave_lstm.pt',\n",
    "    '/tmp/heatwave_lstm.pt'\n",
    "])\n",
    "subprocess.run(['docker', 'cp', 'namenode:/tmp/heatwave_lstm.pt', 'models/'])\n",
    "\n",
    "# Download flood model\n",
    "subprocess.run([\n",
    "    'docker', 'exec', 'namenode', 'hdfs', 'dfs', '-get',\n",
    "    '/user/airflow/models/flood_lstm.pt',\n",
    "    '/tmp/flood_lstm.pt'\n",
    "])\n",
    "subprocess.run(['docker', 'cp', 'namenode:/tmp/flood_lstm.pt', 'models/'])\n",
    "\n",
    "print(\"✅ Models downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66466250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "input_size = X_test_heat.shape[2]  # Number of features\n",
    "\n",
    "# Heatwave model\n",
    "heatwave_model = LSTMModel(input_size=input_size, hidden_size=64, fc_size=32)\n",
    "heatwave_model.load_state_dict(torch.load('models/heatwave_lstm.pt', weights_only=False))\n",
    "heatwave_model.eval()\n",
    "print(\"✅ Heatwave model loaded\")\n",
    "\n",
    "# Flood model\n",
    "flood_model = LSTMModel(input_size=input_size, hidden_size=64, fc_size=32)\n",
    "flood_model.load_state_dict(torch.load('models/flood_lstm.pt', weights_only=False))\n",
    "flood_model.eval()\n",
    "print(\"✅ Flood model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd0805",
   "metadata": {},
   "source": [
    "## 7. Run Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to tensors\n",
    "X_test_heat_tensor = torch.FloatTensor(X_test_heat)\n",
    "X_test_flood_tensor = torch.FloatTensor(X_test_flood)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    # Heatwave predictions\n",
    "    heatwave_probs = heatwave_model(X_test_heat_tensor).squeeze().numpy()\n",
    "    heatwave_preds = (heatwave_probs >= 0.5).astype(int)\n",
    "    \n",
    "    # Flood predictions\n",
    "    flood_probs = flood_model(X_test_flood_tensor).squeeze().numpy()\n",
    "    flood_preds = (flood_probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"✅ Inference completed!\")\n",
    "print(f\"\\nHeatwave predictions: {heatwave_preds.sum()} positive out of {len(heatwave_preds)}\")\n",
    "print(f\"Flood predictions: {flood_preds.sum()} positive out of {len(flood_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b540eaf",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fcfb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatwave model evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"HEATWAVE MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_heat, heatwave_preds, target_names=['No Heatwave', 'Heatwave']))\n",
    "\n",
    "# ROC AUC\n",
    "if len(np.unique(y_test_heat)) > 1:\n",
    "    roc_auc = roc_auc_score(y_test_heat, heatwave_probs)\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FLOOD MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_flood, flood_preds, target_names=['No Flood', 'Flood']))\n",
    "\n",
    "# ROC AUC\n",
    "if len(np.unique(y_test_flood)) > 1:\n",
    "    roc_auc = roc_auc_score(y_test_flood, flood_probs)\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86b7b3",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f8950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatwave confusion matrix\n",
    "cm_heat = confusion_matrix(y_test_heat, heatwave_preds)\n",
    "sns.heatmap(cm_heat, annot=True, fmt='d', cmap='Reds', ax=axes[0],\n",
    "            xticklabels=['No Heatwave', 'Heatwave'],\n",
    "            yticklabels=['No Heatwave', 'Heatwave'])\n",
    "axes[0].set_title('Heatwave Prediction - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Flood confusion matrix\n",
    "cm_flood = confusion_matrix(y_test_flood, flood_preds)\n",
    "sns.heatmap(cm_flood, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['No Flood', 'Flood'],\n",
    "            yticklabels=['No Flood', 'Flood'])\n",
    "axes[1].set_title('Flood Prediction - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df2d30",
   "metadata": {},
   "source": [
    "## 10. ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01331414",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatwave ROC\n",
    "if len(np.unique(y_test_heat)) > 1:\n",
    "    fpr_heat, tpr_heat, _ = roc_curve(y_test_heat, heatwave_probs)\n",
    "    roc_auc_heat = roc_auc_score(y_test_heat, heatwave_probs)\n",
    "    \n",
    "    axes[0].plot(fpr_heat, tpr_heat, color='red', lw=2, label=f'ROC curve (AUC = {roc_auc_heat:.3f})')\n",
    "    axes[0].plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random')\n",
    "    axes[0].set_xlim([0.0, 1.0])\n",
    "    axes[0].set_ylim([0.0, 1.05])\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title('Heatwave Prediction - ROC Curve', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(loc='lower right')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Flood ROC\n",
    "if len(np.unique(y_test_flood)) > 1:\n",
    "    fpr_flood, tpr_flood, _ = roc_curve(y_test_flood, flood_probs)\n",
    "    roc_auc_flood = roc_auc_score(y_test_flood, flood_probs)\n",
    "    \n",
    "    axes[1].plot(fpr_flood, tpr_flood, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_flood:.3f})')\n",
    "    axes[1].plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random')\n",
    "    axes[1].set_xlim([0.0, 1.0])\n",
    "    axes[1].set_ylim([0.0, 1.05])\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].set_title('Flood Prediction - ROC Curve', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(loc='lower right')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e775269",
   "metadata": {},
   "source": [
    "## 11. Prediction Probability Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454adf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatwave probability distribution\n",
    "axes[0].hist(heatwave_probs[y_test_heat == 0], bins=50, alpha=0.6, label='No Heatwave', color='green')\n",
    "axes[0].hist(heatwave_probs[y_test_heat == 1], bins=50, alpha=0.6, label='Heatwave', color='red')\n",
    "axes[0].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
    "axes[0].set_xlabel('Predicted Probability')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Heatwave Prediction Probabilities', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Flood probability distribution\n",
    "axes[1].hist(flood_probs[y_test_flood == 0], bins=50, alpha=0.6, label='No Flood', color='green')\n",
    "axes[1].hist(flood_probs[y_test_flood == 1], bins=50, alpha=0.6, label='Flood', color='blue')\n",
    "axes[1].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Flood Prediction Probabilities', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('probability_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ae890",
   "metadata": {},
   "source": [
    "## 12. Sample Predictions with Actual Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual_Heatwave': y_test_heat,\n",
    "    'Predicted_Heatwave': heatwave_preds,\n",
    "    'Heatwave_Probability': heatwave_probs,\n",
    "    'Actual_Flood': y_test_flood,\n",
    "    'Predicted_Flood': flood_preds,\n",
    "    'Flood_Probability': flood_probs\n",
    "})\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.head(20))\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('inference_results.csv', index=False)\n",
    "print(\"\\n✅ Results saved to 'inference_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710e618",
   "metadata": {},
   "source": [
    "## 13. False Positives and False Negatives Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatwave analysis\n",
    "heat_fp = np.where((y_test_heat == 0) & (heatwave_preds == 1))[0]\n",
    "heat_fn = np.where((y_test_heat == 1) & (heatwave_preds == 0))[0]\n",
    "\n",
    "print(\"HEATWAVE MODEL ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"False Positives: {len(heat_fp)} ({len(heat_fp)/len(y_test_heat)*100:.2f}%)\")\n",
    "print(f\"False Negatives: {len(heat_fn)} ({len(heat_fn)/len(y_test_heat)*100:.2f}%)\")\n",
    "\n",
    "if len(heat_fp) > 0:\n",
    "    print(f\"\\nFalse Positive average probability: {heatwave_probs[heat_fp].mean():.3f}\")\n",
    "if len(heat_fn) > 0:\n",
    "    print(f\"False Negative average probability: {heatwave_probs[heat_fn].mean():.3f}\")\n",
    "\n",
    "# Flood analysis\n",
    "flood_fp = np.where((y_test_flood == 0) & (flood_preds == 1))[0]\n",
    "flood_fn = np.where((y_test_flood == 1) & (flood_preds == 0))[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FLOOD MODEL ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"False Positives: {len(flood_fp)} ({len(flood_fp)/len(y_test_flood)*100:.2f}%)\")\n",
    "print(f\"False Negatives: {len(flood_fn)} ({len(flood_fn)/len(y_test_flood)*100:.2f}%)\")\n",
    "\n",
    "if len(flood_fp) > 0:\n",
    "    print(f\"\\nFalse Positive average probability: {flood_probs[flood_fp].mean():.3f}\")\n",
    "if len(flood_fn) > 0:\n",
    "    print(f\"False Negative average probability: {flood_probs[flood_fn].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b7cfa",
   "metadata": {},
   "source": [
    "## 14. Export Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56fb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "summary = {\n",
    "    'Model': ['Heatwave', 'Flood'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test_heat, heatwave_preds),\n",
    "        accuracy_score(y_test_flood, flood_preds)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test_heat, heatwave_preds, zero_division=0),\n",
    "        precision_score(y_test_flood, flood_preds, zero_division=0)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test_heat, heatwave_preds, zero_division=0),\n",
    "        recall_score(y_test_flood, flood_preds, zero_division=0)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test_heat, heatwave_preds, zero_division=0),\n",
    "        f1_score(y_test_flood, flood_preds, zero_division=0)\n",
    "    ]\n",
    "}\n",
    "\n",
    "if len(np.unique(y_test_heat)) > 1 and len(np.unique(y_test_flood)) > 1:\n",
    "    summary['ROC-AUC'] = [\n",
    "        roc_auc_score(y_test_heat, heatwave_probs),\n",
    "        roc_auc_score(y_test_flood, flood_probs)\n",
    "    ]\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('model_performance_summary.csv', index=False)\n",
    "print(\"\\n✅ Summary saved to 'model_performance_summary.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333dc17",
   "metadata": {},
   "source": [
    "## 15. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263da966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"✅ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
