"""
HDFS Inference Client for Frontend
Fetches predictions from HDFS that were generated by Airflow inference tasks.
Integrates the frontend with the Flood-And-HeatWave-Predictor backend.

Usage:
    from hdfs_inference_client import HDFSInferenceClient
    client = HDFSInferenceClient()
    predictions = client.get_latest_predictions('Kathmandu')

Architecture:
    Frontend â†’ HDFS Inference Client â†’ HDFS (predictions from Airflow)
                                          â†“
                              Latest model checkpoints
"""

import os
import json
import time
import requests
from datetime import datetime
from typing import Optional, Dict, List, Any
import threading
import warnings
warnings.filterwarnings('ignore')

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

try:
    from pyspark.sql import SparkSession
    SPARK_AVAILABLE = True
except ImportError:
    SPARK_AVAILABLE = False

# Configuration
HDFS_NAMENODE = os.environ.get('HDFS_NAMENODE', 'hdfs://namenode:9000')
HDFS_WEBHDFS_URL = os.environ.get('HDFS_WEBHDFS_URL', 'http://localhost:9870')
HDFS_BASE_PATH = os.environ.get('HDFS_BASE_PATH', '/user/airflow/weather_data')
INFERENCE_POLL_INTERVAL = int(os.environ.get('INFERENCE_POLL_INTERVAL', '2'))


class HDFSInferenceClient:
    """
    Client to fetch inference results from HDFS.
    Supports both Spark and WebHDFS REST API access.
    """
    
    def __init__(self,
                 hdfs_namenode: str = HDFS_NAMENODE,
                 webhdfs_url: str = HDFS_WEBHDFS_URL,
                 base_path: str = HDFS_BASE_PATH,
                 use_spark: bool = True):
        
        self.hdfs_namenode = hdfs_namenode
        self.webhdfs_url = webhdfs_url
        self.base_path = base_path
        self.use_spark = use_spark and SPARK_AVAILABLE
        self.spark = None
        self._cache = {}
        self._cache_time = {}
        self._cache_ttl = 5  # seconds
    
    def _get_spark(self):
        """Get or create Spark session."""
        if not SPARK_AVAILABLE:
            return None
        
        if self.spark is None:
            self.spark = SparkSession.builder \
                .appName("HDFSInferenceClient") \
                .master("local[*]") \
                .config("spark.hadoop.fs.defaultFS", self.hdfs_namenode) \
                .getOrCreate()
        
        return self.spark
    
    def get_latest_predictions(self, 
                               district: Optional[str] = None,
                               model_type: str = 'xgb',
                               target: str = 'heatwave',
                               limit: int = 100) -> List[Dict]:
        """
        Get latest predictions from HDFS.
        
        Args:
            district: Filter by district (optional)
            model_type: 'xgb' or 'lstm'
            target: 'heatwave' or 'flood_proxy'
            limit: Maximum number of records
            
        Returns:
            List of prediction dictionaries
        """
        cache_key = f"{model_type}_{target}_{district}_{limit}"
        
        # Check cache
        if cache_key in self._cache:
            if (time.time() - self._cache_time.get(cache_key, 0)) < self._cache_ttl:
                return self._cache[cache_key]
        
        predictions_path = f"{self.base_path}/predictions/{model_type}_{target}"
        
        try:
            if self.use_spark:
                predictions = self._read_with_spark(predictions_path, district, limit)
            else:
                predictions = self._read_with_webhdfs(predictions_path, district, limit)
            
            # Update cache
            self._cache[cache_key] = predictions
            self._cache_time[cache_key] = time.time()
            
            return predictions
            
        except Exception as e:
            print(f"âš ï¸  Error reading predictions: {e}")
            return []
    
    def _read_with_spark(self, 
                         path: str,
                         district: Optional[str],
                         limit: int) -> List[Dict]:
        """Read predictions using Spark."""
        spark = self._get_spark()
        if spark is None:
            return self._read_with_webhdfs(path, district, limit)
        
        try:
            full_path = f"{self.hdfs_namenode}{path}"
            df = spark.read.parquet(full_path)
            
            if district:
                df = df.filter(df.District == district)
            
            # Get latest records
            df = df.orderBy(df.Date.desc()).limit(limit)
            
            # Convert to list of dicts
            return [row.asDict() for row in df.collect()]
            
        except Exception as e:
            print(f"âš ï¸  Spark read error: {e}")
            return []
    
    def _read_with_webhdfs(self,
                           path: str,
                           district: Optional[str],
                           limit: int) -> List[Dict]:
        """Read predictions using WebHDFS REST API."""
        try:
            # List files in directory
            list_url = f"{self.webhdfs_url}/webhdfs/v1{path}?op=LISTSTATUS"
            response = requests.get(list_url, timeout=10)
            
            if response.status_code != 200:
                return []
            
            files = response.json().get('FileStatuses', {}).get('FileStatus', [])
            parquet_files = [f for f in files if f['pathSuffix'].endswith('.parquet')]
            
            if not parquet_files:
                return []
            
            # Note: WebHDFS can't directly read parquet, would need to download
            # For now, return empty and suggest using Spark
            print("â„¹ï¸  WebHDFS cannot read parquet directly. Use Spark or download files.")
            return []
            
        except Exception as e:
            print(f"âš ï¸  WebHDFS error: {e}")
            return []
    
    def get_all_predictions(self, 
                           district: Optional[str] = None,
                           limit: int = 100) -> Dict[str, List[Dict]]:
        """
        Get predictions from all models and targets.
        
        Returns:
            Dictionary with keys like 'xgb_heatwave', 'lstm_flood_proxy', etc.
        """
        results = {}
        
        for model_type in ['xgb', 'lstm', 'rule']:
            for target in ['heatwave', 'flood_proxy']:
                key = f"{model_type}_{target}"
                predictions = self.get_latest_predictions(
                    district=district,
                    model_type=model_type,
                    target=target,
                    limit=limit
                )
                if predictions:
                    results[key] = predictions
        
        return results
    
    def get_prediction_summary(self, district: Optional[str] = None) -> Dict:
        """
        Get summary of latest predictions.
        
        Returns:
            Summary dictionary with risk levels and probabilities
        """
        all_preds = self.get_all_predictions(district=district, limit=1)
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'district': district,
            'heatwave': {
                'xgb_probability': None,
                'lstm_probability': None,
                'risk': 'UNKNOWN'
            },
            'flood': {
                'xgb_probability': None,
                'lstm_probability': None,
                'risk': 'UNKNOWN'
            }
        }
        
        # Extract heatwave predictions
        if 'xgb_heatwave' in all_preds and all_preds['xgb_heatwave']:
            pred = all_preds['xgb_heatwave'][0]
            summary['heatwave']['xgb_probability'] = pred.get('heatwave_probability')
            summary['heatwave']['risk'] = pred.get('heatwave_risk', 'UNKNOWN')
        
        if 'lstm_heatwave' in all_preds and all_preds['lstm_heatwave']:
            pred = all_preds['lstm_heatwave'][0]
            summary['heatwave']['lstm_probability'] = pred.get('heatwave_probability')
        
        # Extract flood predictions
        if 'xgb_flood_proxy' in all_preds and all_preds['xgb_flood_proxy']:
            pred = all_preds['xgb_flood_proxy'][0]
            summary['flood']['xgb_probability'] = pred.get('flood_proxy_probability')
            summary['flood']['risk'] = pred.get('flood_proxy_risk', 'UNKNOWN')
        
        if 'lstm_flood_proxy' in all_preds and all_preds['lstm_flood_proxy']:
            pred = all_preds['lstm_flood_proxy'][0]
            summary['flood']['lstm_probability'] = pred.get('flood_proxy_probability')
        
        return summary
    
    def close(self):
        """Close Spark session."""
        if self.spark:
            self.spark.stop()
            self.spark = None


class InferencePollService:
    """
    Service that polls HDFS for new predictions at regular intervals.
    Useful for real-time dashboard updates.
    """
    
    def __init__(self,
                 poll_interval: int = INFERENCE_POLL_INTERVAL,
                 callback = None):
        
        self.poll_interval = poll_interval
        self.callback = callback
        self.client = HDFSInferenceClient()
        self.running = False
        self._thread = None
        self.latest_predictions = {}
    
    def start(self):
        """Start polling service in background thread."""
        self.running = True
        self._thread = threading.Thread(target=self._poll_loop, daemon=True)
        self._thread.start()
        print(f"ðŸ”„ Inference poll service started (interval: {self.poll_interval}s)")
    
    def stop(self):
        """Stop polling service."""
        self.running = False
        if self._thread:
            self._thread.join(timeout=5)
        self.client.close()
        print("â¹ï¸  Inference poll service stopped")
    
    def _poll_loop(self):
        """Main polling loop."""
        while self.running:
            try:
                predictions = self.client.get_all_predictions(limit=10)
                self.latest_predictions = predictions
                
                if self.callback:
                    self.callback(predictions)
                
            except Exception as e:
                print(f"âš ï¸  Poll error: {e}")
            
            time.sleep(self.poll_interval)
    
    def get_latest(self) -> Dict:
        """Get latest polled predictions."""
        return self.latest_predictions.copy()


# Rule-based fallback predictor (when HDFS predictions unavailable)
class FallbackPredictor:
    """
    Fallback rule-based predictor when HDFS is unavailable.
    """
    
    @staticmethod
    def predict_heatwave(max_temp: float, humidity: float = 50) -> Dict:
        """Predict heatwave risk based on temperature."""
        probability = max(0, min(1, (max_temp - 35) / 15))
        risk = 'HIGH' if probability > 0.5 else 'MEDIUM' if probability > 0.3 else 'LOW'
        
        return {
            'probability': probability,
            'risk': risk,
            'method': 'rule-based'
        }
    
    @staticmethod
    def predict_flood(precipitation: float, humidity: float = 50) -> Dict:
        """Predict flood risk based on precipitation and humidity."""
        probability = max(0, min(1, 
            (precipitation - 50) / 100 + (humidity - 80) / 100
        ))
        risk = 'HIGH' if probability > 0.5 else 'MEDIUM' if probability > 0.3 else 'LOW'
        
        return {
            'probability': probability,
            'risk': risk,
            'method': 'rule-based'
        }
    
    @staticmethod
    def predict_all(weather_data: Dict) -> Dict:
        """Make all predictions from weather data."""
        max_temp = weather_data.get('MaxTemp_2m', weather_data.get('max_temp', 30))
        precip = weather_data.get('Precip', weather_data.get('precipitation', 0))
        humidity = weather_data.get('RH_2m', weather_data.get('humidity', 50))
        
        return {
            'heatwave': FallbackPredictor.predict_heatwave(max_temp, humidity),
            'flood': FallbackPredictor.predict_flood(precip, humidity),
            'timestamp': datetime.now().isoformat(),
            'source': 'fallback'
        }


def main():
    """Test the HDFS inference client."""
    print("=" * 60)
    print("HDFS Inference Client Test")
    print("=" * 60)
    
    client = HDFSInferenceClient()
    
    # Test getting predictions
    print("\nFetching predictions...")
    
    for target in ['heatwave', 'flood_proxy']:
        for model_type in ['xgb', 'lstm']:
            preds = client.get_latest_predictions(
                model_type=model_type,
                target=target,
                limit=5
            )
            print(f"\n{model_type.upper()} {target}: {len(preds)} predictions")
            if preds:
                print(f"  Latest: {preds[0]}")
    
    # Test summary
    print("\n" + "=" * 60)
    print("Prediction Summary")
    print("=" * 60)
    
    summary = client.get_prediction_summary()
    print(json.dumps(summary, indent=2, default=str))
    
    # Test fallback
    print("\n" + "=" * 60)
    print("Fallback Predictor Test")
    print("=" * 60)
    
    test_data = {
        'MaxTemp_2m': 42.0,
        'Precip': 75.0,
        'RH_2m': 85.0
    }
    
    fallback_result = FallbackPredictor.predict_all(test_data)
    print(json.dumps(fallback_result, indent=2))
    
    client.close()
    print("\nâœ… Test completed")


if __name__ == "__main__":
    main()
